# -*- coding: utf-8 -*-
"""kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LLVDKa4AfudTqyKQSLVkJyg5ZU-AqkWw
"""
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVR
from boruta import BorutaPy
from sklearn import preprocessing
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.feature_selection import RFE
import numpy as np
import xgboost as xgb

dataset = pd.read_csv("train.csv")

y = dataset['SalePrice'].values
X = dataset.drop('SalePrice', axis=1)
le = preprocessing.LabelEncoder()

for column in X:
  if X[column].isnull().values.any():
    if X[column].dtype == 'float64':
      X[column].fillna(-1.0, inplace=True)
    else:
      X[column].fillna('replaced', inplace=True)
      X[[column]] = X[[column]].apply(le.fit_transform)

X = X.apply(le.fit_transform)

x_labels = X.columns[1:]
X = X.values[:, 1:]

# layer1_clfs = [
#         RandomForestClassifier(n_estimators=50, max_depth=6, n_jobs=-1, criterion='gini'),
#         RandomForestClassifier(n_estimators=50, max_depth=6, n_jobs=-1, criterion='entropy'),
#         ExtraTreesClassifier(n_estimators=50, max_depth=6, n_jobs=-1, criterion='gini'),
#         ExtraTreesClassifier(n_estimators=50, max_depth=6, n_jobs=-1, criterion='entropy'),
#         # GradientBoostingClassifier(learning_rate=0.05, max_depth=6, n_estimators=70),
#         # xgb.XGBRegressor(objective ='reg:squarederror', booster="gbtree"),
#         # xgb.XGBRegressor(objective ='reg:squarederror', booster="gblinear"),
#         # KernelRidge(),
#         # SVR(),
#         # BayesianRidge(),
#         ]
#
# index_to_delete = set()
# for clf in layer1_clfs:
#     selector = RFE(clf, n_features_to_select=70, step=1)
#     selector = selector.fit(X, y)
#     for index, ranking in enumerate(selector.ranking_):
#         if ranking > 1:
#             index_to_delete.add(index)
#
# print(index_to_delete)
# index_selected = list(range(X.shape[1]))
# for elem in index_to_delete:
#     index_selected.remove(elem)
# print(index_selected)


# ranking = np.array([43, 31, 14,  5, 52, 76, 67, 65, 79, 69, 62, 17, 60, 25, 63, 54, 15,
#        21,  7, 13, 58, 70, 34, 30, 20,  4, 48, 73, 41, 41, 60, 41, 38, 12,
#        38, 22,  3,  8, 26, 73, 76, 73,  2, 11, 46,  1, 50, 47, 28, 44, 19,
#        78, 56, 16, 53, 44, 36, 51,  9, 66, 28,  6, 57, 68, 76,  9, 17, 23,
#        60, 31, 37, 48, 71, 33, 24, 27, 34, 63, 54])
# features_selected = []
# index_selected = []
# counter = 0
# # for columns in dataset:
# #   print(columns)
# for index, label in enumerate(x_labels):
#   if ranking[index] <= 50:
#     features_selected.append(label)
#     index_selected.append(index)

index_selected = [0, 1, 2, 3, 6, 9, 11, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 72, 75, 76, 77, 78]
index_selected = list(range(X.shape[1]))

X = X[:, index_selected]

skf = StratifiedKFold(n_splits=4)
skf.get_n_splits(X, y)

layer1_clfs = [
        # RandomForestClassifier(n_estimators=50, max_depth=6, n_jobs=-1, criterion='gini'),
        # RandomForestClassifier(n_estimators=50, max_depth=6, n_jobs=-1, criterion='entropy'),
        # ExtraTreesClassifier(n_estimators=50, max_depth=6, n_jobs=-1, criterion='gini'),
        # ExtraTreesClassifier(n_estimators=50, max_depth=6, n_jobs=-1, criterion='entropy'),
        # xgb.XGBRegressor(objective ='reg:squarederror', booster="gbtree"),
        # xgb.XGBRegressor(objective ='reg:squarederror', booster="gblinear"),
        KNeighborsClassifier(2),
        # KernelRidge(),
        # BayesianRidge(),
        # Lasso(),
        ]
layer2_clfs = [
        xgb.XGBRegressor(objective ='reg:squarederror', booster="gbtree"),
        KernelRidge(),
        ]

def my_train(X, y):
  ret = {'layer1': [], 'layer2': [], 'layer3': []}
  layer1_blend_train = np.zeros((X.shape[0], len(layer1_clfs)))
  layer2_blend_train = np.zeros((layer1_blend_train.shape[0], len(layer2_clfs)))

  # layer 1
  for j, clf in enumerate(layer1_clfs):
    clf.fit(X, y)
    layer1_output = clf.predict(X)
    layer1_blend_train[:, j] = layer1_output
    ret['layer1'].append(clf)
  # layer 2
  for j, clf in enumerate(layer2_clfs):
    clf.fit(layer1_blend_train, y)
    layer2_output = clf.predict(layer1_blend_train)
    layer2_blend_train[:, j] = layer2_output
    ret['layer2'].append(clf)
  # final layer
  final_linear = KernelRidge()
  final_linear.fit(layer2_blend_train, y)
  final_output = final_linear.predict(layer2_blend_train)
  final_score = sqrt(mean_squared_error(np.log(y), np.log(final_output)))
  ret['layer3'].append(final_linear)
  return ret, final_output

def my_test(X, ret):
  layer1_blend_train = np.zeros((X.shape[0], len(layer1_clfs)))
  layer2_blend_train = np.zeros((layer1_blend_train.shape[0], len(layer2_clfs)))

  # layer 1
  for j, clf in enumerate(ret['layer1']):
    layer1_output = clf.predict(X)
    layer1_blend_train[:, j] = layer1_output
  # layer 2
  for j, clf in enumerate(ret['layer2']):
    layer2_output = clf.predict(layer1_blend_train)
    layer2_blend_train[:, j] = layer2_output
  # final layer
  final_linear = ret['layer3'][0]
  final_output = final_linear.predict(layer2_blend_train)
  return final_output

def remove_outliers():
  my_model, prediction = my_train(X, y)
  abs_diff = np.abs(prediction, y.astype(float))
  mean_abs_diff = np.mean(abs_diff)
  std_abs_diff = np.std(abs_diff)
  index_to_delete = []
  for i in range(len(X)):
    if (abs_diff[i] - mean_abs_diff) / std_abs_diff > 4:
      index_to_delete.append(i)

  index_to_keep = [x for x in range(len(X)) if x not in index_to_delete]
  ret_X = X[index_to_keep]
  ret_y = y[index_to_keep]
  return ret_X, ret_y

ret_X, ret_y = remove_outliers()
score = 0.0
count = 0
for train, test in skf.split(ret_X, ret_y):
      X_train = ret_X[train]
      y_train = ret_y[train]
      X_test = ret_X[test]
      y_test = ret_y[test]
      my_model, train_prediction = my_train(X_train, y_train)
      test_prediction = my_test(X_test, my_model)
      train_score = sqrt(mean_squared_error(np.log(y_train), np.log(train_prediction)))
      test_score = sqrt(mean_squared_error(np.log(y_test), np.log(test_prediction)))
      print("round: {}, train score: {}, test score: {}".format(count, train_score, test_score))
      score += test_score / 4
      count += 1
print("Score: {}".format(score))

# test_X = pd.read_csv("test.csv")
# for column in test_X:
#   if test_X[column].isnull().values.any():
#     if test_X[column].dtype == 'float64':
#       test_X[column].fillna(-1.0, inplace=True)
#     else:
#       test_X[column].fillna('replaced', inplace=True)
#       test_X[[column]] = test_X[[column]].apply(le.fit_transform)
# test_X = test_X.apply(le.fit_transform)
#
# test_X = test_X.values[:, 1:]
# test_X = test_X[:, index_selected]
#
# my_model, output = my_train(ret_X, ret_y)
# y_test = my_test(test_X, my_model)
# print(sqrt(mean_squared_error(np.log(ret_y), np.log(output))))
#
# output = pd.DataFrame()
# output[['Id']] = pd.read_csv("test.csv")[['Id']]
# output['SalePrice'] = y_test
# output.to_csv('submission.csv', index=False)
